{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae11ff5d-ed6e-402f-8d0a-9af8c09265f9",
   "metadata": {},
   "source": [
    "# ESG Report Generator\n",
    "# Project Overview\n",
    "\n",
    "This project generates **company-specific ESG (Environmental, Social) reports** using a combination of structured data (Excel), unstructured data (DOCX), and online sources (via Tavily search).  \n",
    "The reports are written in **Markdown** and exported to **PDF** with professional formatting.\n",
    "\n",
    "## Approach\n",
    "\n",
    "1. **Data Ingestion**  \n",
    "   - Company descriptions are extracted from a DOCX file.  \n",
    "   - ESG insights and indicators are read from an Excel sheet.  \n",
    "   - Additional online ESG information is retrieved using the Tavily API.  \n",
    "\n",
    "2. **Document Construction**  \n",
    "   - All available information (DOCX, XLSX, and online search) is combined into a unified set of documents.  \n",
    "   - Texts are chunked into smaller pieces using LangChain’s `RecursiveCharacterTextSplitter`.  \n",
    "   - Each chunk is embedded into a vector representation and stored in a FAISS vector database.  \n",
    "\n",
    "3. **Planning and Section Generation**  \n",
    "   - A planning step (prompted via Azure OpenAI LLM) determines the sections of the report, aligned with ESRS standards.  \n",
    "   - Each section includes **goals**, a **target length**, and possible **tables**.  \n",
    "   - The system retrieves relevant evidence from the vector store for each section.  \n",
    "   - The LLM writes the section content, explicitly addressing **actual impacts** and **potential impacts**, and provides **gaps** and **recommendations**.  \n",
    "\n",
    "4. **Report Assembly**  \n",
    "   - All sections are combined into a single structured Markdown report.  \n",
    "   - References are automatically attached to the sections based on the evidence used.  \n",
    "   - An appendix is added describing methodology and limitations.  \n",
    "\n",
    "5. **Export**  \n",
    "   - The final Markdown is saved locally.  \n",
    "   - The report is also exported as a styled **PDF** using WeasyPrint.  \n",
    "\n",
    "## Key Features\n",
    "\n",
    "- Retrieval-Augmented Generation (RAG) for evidence-based report writing.  \n",
    "- Integration of both **offline data (DOCX/XLSX)** and **online search (Tavily)**.  \n",
    "- Deterministic section planning aligned with ESRS environmental and social standards.  \n",
    "- Dual output formats: **Markdown (editable)** and **PDF (ready to share)**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0067ca6-24a3-41c4-bab6-50814528c36c",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "We import standard Python utilities, data processing tools, LangChain components for LLM and vector stores, Tavily for online search, and libraries for generating PDF output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b6fe5dc-2af4-40cc-8ffe-9ef621f5fa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup and imports ---\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import uuid\n",
    "from typing import List, Dict, TypedDict, Optional\n",
    "\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document as LCDocument\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from tavily import TavilyClient\n",
    "from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer\n",
    "from reportlab.lib.styles import getSampleStyleSheet\n",
    "from reportlab.lib.pagesizes import A4\n",
    "import markdown as md\n",
    "from weasyprint import HTML, CSS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1392b8-0f03-46d8-9261-bf90797c78b7",
   "metadata": {},
   "source": [
    "## Online ESG Search\n",
    "\n",
    "Fetch ESG-related documents for a company across given topics using the Tavily API.  \n",
    "Results are wrapped as `LCDocument` objects with metadata (company, source, URL, title).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b1549a9-fcdb-4631-8464-80c55458d526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_online_esg(company_name: str, topics: List[str], max_results: int = 5) -> List[LCDocument]:\n",
    "    docs = []\n",
    "    for topic in topics:\n",
    "        query = f\"{company_name} ESG {topic}\"\n",
    "        try:\n",
    "            res = tavily_client.search(query=query, num_results=max_results)\n",
    "            for r in res[\"results\"]:\n",
    "                content = r.get(\"content\", \"\") or r.get(\"snippet\", \"\")\n",
    "                if content.strip():\n",
    "                    docs.append(\n",
    "                        LCDocument(\n",
    "                            page_content=content,\n",
    "                            metadata={\n",
    "                                \"company\": company_name,\n",
    "                                \"source\": \"tavily\",\n",
    "                                \"url\": r.get(\"url\", \"\"),\n",
    "                                \"title\": r.get(\"title\", topic)\n",
    "                            }\n",
    "                        )\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Tavily search failed for query: {query}, error: {e}\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe71462-fb53-4c7c-8cd5-048ea749da60",
   "metadata": {},
   "source": [
    "## Extract Company Descriptions\n",
    "\n",
    "Parse the DOCX file and collect company names with their description text into a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9adb4d7b-8da5-487e-9a14-740d433ce1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_descriptions(docx_path: Path) -> Dict[str, str]:\n",
    "    doc = Document(docx_path)\n",
    "    companies, current_company, buffer = {}, None, []\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        if text.startswith(\"Company\"):\n",
    "            if current_company and buffer:\n",
    "                companies[current_company] = \"\\n\".join(buffer).strip()\n",
    "                buffer = []\n",
    "            if \":\" in text:\n",
    "                current_company = text.split(\":\", 1)[1].strip()\n",
    "        else:\n",
    "            buffer.append(text)\n",
    "    if current_company and buffer:\n",
    "        companies[current_company] = \"\\n\".join(buffer).strip()\n",
    "    return companies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a0e171-3538-4d67-a6a2-4c4d831d858b",
   "metadata": {},
   "source": [
    "## Load Excel Data\n",
    "\n",
    "Read the first sheet of the XLSX file into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a377556d-e4ab-4fe5-a841-3684d84fce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_sheet(xlsx_path: Path) -> pd.DataFrame:\n",
    "    df_all = pd.read_excel(xlsx_path, sheet_name=None)\n",
    "    first = list(df_all.keys())[0]\n",
    "    return df_all[first].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b4a1b-3e30-44ba-af62-86ff24aa4953",
   "metadata": {},
   "source": [
    "## Normalize Name\n",
    "\n",
    "Convert a name to lowercase and remove extra spaces for consistent matching.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e6b4442-7df7-4d89-95c0-5ef8b07a2712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \"\", name.strip().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b14ff9d-1b7e-4d1b-bb45-f7e259fffc3e",
   "metadata": {},
   "source": [
    "## Build Documents\n",
    "\n",
    "Combine all available information for a company into document objects:\n",
    "- Company description from DOCX  \n",
    "- Related rows from the Excel insights file  \n",
    "- ESG topics retrieved online via Tavily  \n",
    "The result is a list of `LCDocument` objects with metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9310d11-5857-42ec-b8e0-f99ebe1be616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_documents(company_name: str, description: str, insights_df: pd.DataFrame) -> List[LCDocument]:\n",
    "    docs = [LCDocument(page_content=description, metadata={\"company\": company_name, \"source\": \"docx\"})]\n",
    "\n",
    "    # insights\n",
    "    if \"company\" in insights_df.columns:\n",
    "        mask = insights_df[\"company\"].str.lower().str.contains(normalize_name(company_name), na=False) \\\n",
    "               | insights_df[\"company\"].str.lower().eq(company_name.lower())\n",
    "        subset = insights_df[mask].copy()\n",
    "    else:\n",
    "        subset = pd.DataFrame()\n",
    "\n",
    "    for i, r in enumerate(subset.itertuples(index=False)):\n",
    "        parts = []\n",
    "        if hasattr(r, \"name\") and getattr(r, \"name\"):\n",
    "            parts.append(f\"{getattr(r,'name')}\")\n",
    "        if hasattr(r, \"description\") and getattr(r, \"description\"):\n",
    "            parts.append(f\"{getattr(r,'description')}\")\n",
    "        if hasattr(r, \"context\") and isinstance(getattr(r,\"context\"), str) and getattr(r,\"context\").strip():\n",
    "            parts.append(getattr(r,\"context\").strip())\n",
    "        content = \" | \".join(p for p in parts if p)\n",
    "        md = {\n",
    "            \"company\": company_name,\n",
    "            \"source\": f\"xlsx-{i}\",\n",
    "            \"file_name\": getattr(r, \"file_name\", \"\"),\n",
    "            \"page\": getattr(r, \"page_nbr\", \"\"),\n",
    "        }\n",
    "        if content:\n",
    "            docs.append(LCDocument(page_content=content, metadata=md))\n",
    "\n",
    "    # online ESG data\n",
    "    esg_topics = [\"climate change\", \"pollution\", \"water use\", \"biodiversity\", \"circular economy\",\n",
    "                  \"workforce\", \"supply chain labor\", \"communities\", \"consumers\"]\n",
    "    online_docs = search_online_esg(company_name, esg_topics, max_results=3)\n",
    "    docs.extend(online_docs)\n",
    "\n",
    "    return docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898aa2e9-6566-40dc-a7fe-49e4bbb16f6a",
   "metadata": {},
   "source": [
    "## Build or Load Vector Store\n",
    "\n",
    "Split documents into chunks, remove duplicates, and either load an existing FAISS vector store or create and save a new one with embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3381dd0d-8b54-49ac-9cce-4e16fd4da44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_or_load_vs(company_name: str, docs: List[LCDocument], emb: AzureOpenAIEmbeddings) -> FAISS:\n",
    "    vs_path = VSTORE_DIR / f\"vs_{normalize_name(company_name)}\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1500, chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    splits = splitter.split_documents(docs)\n",
    "    # Lightweight dedup by content hash\n",
    "    seen = set(); unique = []\n",
    "    for d in splits:\n",
    "        h = hash(d.page_content.strip())\n",
    "        if h not in seen:\n",
    "            seen.add(h); unique.append(d)\n",
    "\n",
    "    if vs_path.exists():\n",
    "        store = FAISS.load_local(vs_path, emb, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        store = FAISS.from_documents(unique, emb)\n",
    "        store.save_local(vs_path)\n",
    "    return store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dd055a-7a11-4862-b369-dd8e8268912b",
   "metadata": {},
   "source": [
    "## Summarize Documents for Planning\n",
    "\n",
    "Retrieve a small set of relevant chunks from the vector store and create a short summary string for section planning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f825cdd4-186e-485b-92d9-416979d986a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_docs_for_planning(vs: FAISS, company_query: str = \"high level ESG context and industry positioning\") -> str:\n",
    "    # Shallow retrieval to create a planning summary\n",
    "    docs = vs.similarity_search(company_query, k=8)\n",
    "    text = \"\\n\".join(d.page_content for d in docs)\n",
    "    return text[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163accd-37e8-419e-a3e1-a6e2ed1fec31",
   "metadata": {},
   "source": [
    "## Plan Report Sections\n",
    "\n",
    "Use the LLM to propose an ESG report outline (sections, goals, tables) based on company context.  \n",
    "The output is parsed from JSON with defaults applied if missing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1fd90b-c994-4599-8087-933ff7166cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plan_sections(company_name: str, industry_hint: str, vs: FAISS) -> Dict:\n",
    "    summ = summarize_docs_for_planning(vs)\n",
    "    plan_msg = SECTION_PLANNER_TMPL.format(\n",
    "        company_name=company_name,\n",
    "        industry_hint=industry_hint,\n",
    "        doc_summary=summ\n",
    "    )\n",
    "    resp = llm.invoke(plan_msg)\n",
    "    # Try JSON extraction\n",
    "    m = re.search(r\"\\{.*\\}\", resp.content, flags=re.S)\n",
    "    plan_json = json.loads(m.group(0)) if m else {\"sections\": [], \"tables\": []}\n",
    "    # Safety defaults\n",
    "    for s in plan_json.get(\"sections\", []):\n",
    "        s.setdefault(\"tokens_target\", 900)\n",
    "        s.setdefault(\"goals\", [])\n",
    "    return plan_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5937c7fb-942e-491c-a639-a79326440362",
   "metadata": {},
   "source": [
    "## Retrieve Evidence for a Section\n",
    "\n",
    "Generate search queries with the LLM, run similarity search in the vector store, and return a diverse set of unique supporting documents for the section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09a84c39-3950-408a-b816-6e7cf3271e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_for_section(vs: FAISS, section_title: str, goals: List[str]) -> List[LCDocument]:\n",
    "    qa = QUERY_ANALYZER_TMPL.format(section_title=section_title, goals=\"\\n\".join(goals))\n",
    "    resp = llm.invoke(qa)\n",
    "    try:\n",
    "        spec = json.loads(re.search(r\"\\{.*\\}\", resp.content, flags=re.S).group(0))\n",
    "    except Exception:\n",
    "        spec = {\"queries\": [section_title], \"filters\": {}}\n",
    "\n",
    "    results = []\n",
    "    for q in spec.get(\"queries\", [])[:3]:\n",
    "        results.extend(vs.similarity_search(q, k=6))\n",
    "    # Simple rerank by unique content and source diversity\n",
    "    uniq, seen = [], set()\n",
    "    for d in results:\n",
    "        key = (hash(d.page_content), d.metadata.get(\"source\",\"\"))\n",
    "        if key not in seen:\n",
    "            seen.add(key); uniq.append(d)\n",
    "    return uniq[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44887e-9a07-482a-8104-b3464328081f",
   "metadata": {},
   "source": [
    "## Write Report Section\n",
    "\n",
    "Retrieve supporting evidence for the section, format a prompt with goals and table plan, and use the LLM to generate the Markdown content.  \n",
    "Returns the written section and the documents used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40e326a0-f494-463a-b632-d84d2658fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_section(company: str, section: Dict, vs: FAISS, tables_plan: List[Dict]) -> str:\n",
    "    docs = retrieve_for_section(vs, section[\"title\"], section.get(\"goals\", []))\n",
    "    evidence = \"\\n\\n\".join(f\"- {d.page_content}\" for d in docs)\n",
    "    tables_ok = [t for t in tables_plan if section[\"title\"].lower() in t.get(\"purpose\",\"\").lower() or section[\"title\"].lower() in t.get(\"title\",\"\").lower()]\n",
    "    msg = WRITER_TMPL.format(\n",
    "        company=company,\n",
    "        section=section[\"title\"],\n",
    "        goals=\"\\n\".join(section.get(\"goals\", [])),\n",
    "        evidence=evidence[:8000],\n",
    "        tables_plan=json.dumps(tables_ok[:2]),\n",
    "        tokens_target=section.get(\"tokens_target\", 900)\n",
    "    )\n",
    "    out = llm.invoke(msg).content\n",
    "    return out, docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736ed4d-36f7-45e3-bbf7-55d4b53e3af2",
   "metadata": {},
   "source": [
    "## Build Full Report\n",
    "\n",
    "Generate the complete ESG report for a company:\n",
    "- Create section plan with LLM  \n",
    "- Write each section with evidence and optional tables  \n",
    "- Append references and an appendix  \n",
    "- Return the assembled Markdown report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1faba24d-28ae-41ec-b0dc-b3b68a6d4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_report(company_name: str, vs: FAISS, industry_hint: str = \"Unknown\") -> str:\n",
    "    plan = plan_sections(company_name, industry_hint, vs)\n",
    "    sections = plan.get(\"sections\", [])\n",
    "    tables_plan = plan.get(\"tables\", [])\n",
    "\n",
    "    md = [f\"# ESG Report, {company_name}\", \"\", \"## Contents\"]\n",
    "    for i, s in enumerate(sections, 1):\n",
    "        anchor = re.sub(r'[^a-z0-9]+', '-', s[\"title\"].strip().lower())\n",
    "        md.append(f\"{i}. [{s['title']}](#{anchor})\")\n",
    "    md.append(\"\")\n",
    "\n",
    "    all_refs = []\n",
    "    for s in sections:\n",
    "        # md.append(f\"## {s['title']}\")\n",
    "        body, used_docs = write_section(company_name, s, vs, tables_plan)\n",
    "        md.append(body.strip())\n",
    "        # refs\n",
    "        # refs = []\n",
    "        # for idx, d in enumerate(used_docs, 1):\n",
    "        #     refs.append(f\"[{idx}] {d.metadata.get('file_name','')}, p.{d.metadata.get('page','')}, {d.metadata.get('source','')}\")\n",
    "        refs = []\n",
    "        for idx, d in enumerate(used_docs, 1):\n",
    "            if d.metadata.get(\"source\") == \"tavily\":\n",
    "                refs.append(\n",
    "                    f\"[{idx}] {d.metadata.get('title', 'online source')} ({d.metadata.get('url', '')}) — Tavily\")\n",
    "            else:\n",
    "                refs.append(\n",
    "                    f\"[{idx}] {d.metadata.get('file_name', '')}, p.{d.metadata.get('page', '')}, {d.metadata.get('source', '')}\")\n",
    "\n",
    "        if refs:\n",
    "            md.append(\"\")\n",
    "            md.append(\"### References\")\n",
    "            md.extend(refs)\n",
    "            all_refs.extend(refs)\n",
    "        md.append(\"\")\n",
    "\n",
    "    md.append(\"## Appendix, Methodology and Limitations\")\n",
    "    md.append(\"This report uses retrieval augmented generation, context limited to provided sources, \"\n",
    "              \"length control per section, and ESRS framing for actual and potential impacts. \"\n",
    "              \"If the source evidence is incomplete, recommendations prioritize data improvement.\")\n",
    "\n",
    "    return \"\\n\".join(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04901800-a105-42fa-81d0-d73a856ddd5a",
   "metadata": {},
   "source": [
    "## Save Report (Markdown)\n",
    "\n",
    "Write the generated report to a Markdown file and return the file path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa2a2bf-b36d-4fb1-bdc4-791696a0783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report(company_name: str, markdown: str) -> Path:\n",
    "    out_path = REPORTS_DIR / f\"report_{normalize_name(company_name)}.md\"\n",
    "    out_path.write_text(markdown, encoding=\"utf-8\")\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0837537-2515-4fa5-9d03-c97d922249ed",
   "metadata": {},
   "source": [
    "## Save Report (PDF)\n",
    "\n",
    "Convert the Markdown report to styled HTML, render it with WeasyPrint, and save as a PDF file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d3ab840-b95e-42c8-8ad8-bb3376916cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_report_pdf(company_name: str, markdown_text: str) -> Path:\n",
    "    out_path = REPORTS_DIR / f\"report_{normalize_name(company_name)}.pdf\"\n",
    "\n",
    "    html_text = md.markdown(\n",
    "        markdown_text,\n",
    "        extensions=[\"tables\", \"fenced_code\"]\n",
    "    )\n",
    "\n",
    "    css = CSS(string=\"\"\"\n",
    "        h1 { font-size: 22pt; font-weight: bold; margin-bottom: 10pt; }\n",
    "        h2 { font-size: 18pt; font-weight: bold; margin-top: 15pt; margin-bottom: 8pt; }\n",
    "        h3 { font-size: 14pt; font-weight: bold; margin-top: 12pt; margin-bottom: 6pt; }\n",
    "        p { font-size: 11pt; line-height: 1.5; }\n",
    "        ul, ol { margin-left: 20pt; }\n",
    "        table { border-collapse: collapse; width: 100%; margin: 12pt 0; }\n",
    "        th, td { border: 1px solid #666; padding: 6pt; font-size: 10pt; }\n",
    "    \"\"\")\n",
    "\n",
    "    HTML(string=html_text).write_pdf(str(out_path), stylesheets=[css])\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8732221a-c94a-4d7c-9510-8bb095ac78d6",
   "metadata": {},
   "source": [
    "## Directory Setup\n",
    "\n",
    "Create paths for data, reports, and vector stores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7288220-e391-4474-bb48-a45862d9b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "REPORTS_DIR = Path(\"reports\"); REPORTS_DIR.mkdir(exist_ok=True)\n",
    "VSTORE_DIR = Path(\"vectorstores\"); VSTORE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431dfe8e-df90-4c78-a5e3-431888cbaf21",
   "metadata": {},
   "source": [
    "## Environment Variables\n",
    "\n",
    "Load Azure OpenAI and Tavily API settings from environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51e2ddea-6f18-4db3-8f5e-e080083c13fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "AZURE_ENDPOINT = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "AZURE_API_KEY = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "AZURE_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "DEPLOYMENT_LLM = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_LLM\")\n",
    "DEPLOYMENT_EMB = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_EMBEDDING\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0041219-9aa8-4cb4-9955-e68a0e705a2b",
   "metadata": {},
   "source": [
    "## Model and API Clients\n",
    "\n",
    "Initialize the Azure OpenAI LLM, embeddings model, and Tavily client for online ESG search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83e68032-b15f-425f-b8cb-4a326660bd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=DEPLOYMENT_LLM,\n",
    "    openai_api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    openai_api_version=AZURE_API_VERSION,\n",
    "    model_kwargs={\"max_completion_tokens\": 3000}\n",
    ")\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_deployment=DEPLOYMENT_EMB,\n",
    "    openai_api_key=AZURE_API_KEY,\n",
    "    azure_endpoint=AZURE_ENDPOINT,\n",
    "    openai_api_version=AZURE_API_VERSION,\n",
    ")\n",
    "\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b7f6c-d4f4-461f-80ab-0d470fc112c6",
   "metadata": {},
   "source": [
    "## Section Planner Prompt\n",
    "\n",
    "Prompt template for the LLM to design a structured ESG report outline (sections, goals, tables) based on company context and ESRS standards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab54d87d-6957-47b4-adfc-faa7ed8d5930",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECTION_PLANNER_TMPL = PromptTemplate(\n",
    "    input_variables=[\"company_name\",\"industry_hint\",\"doc_summary\"],\n",
    "    template=(\n",
    "        \"You are an ESG reporting planner. Propose a stable, company-specific outline for a 4 to 6 page Markdown report, focusing ONLY on environmental and social impacts. o NOT include governance.  \"\n",
    "        \"aligned with ESRS. Include a numbered list of sections with titles and 2 to 5 bullet subpoints each. \"\n",
    "        \"Emphasize both actual impacts and potential impacts for environmental and social topics. \"\n",
    "        \"Also include a table plan and where to place it. Keep it deterministic and avoid reusing identical text.\\n\\n\"\n",
    "        \"Company: {company_name}\\n\"\n",
    "        \"Industry hint: {industry_hint}\\n\"\n",
    "        \"Context summary:\\n{doc_summary}\\n\\n\"\n",
    "        \"Return JSON with keys: sections, each item has title, goals, tokens_target; tables, each item has title, purpose, fields.\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee508b7-11f1-44fe-9fae-bd1f4f04cf6b",
   "metadata": {},
   "source": [
    "## Writer Prompt\n",
    "\n",
    "Prompt template for the LLM to generate each report section in Markdown, covering actual and potential impacts, evidence, gaps, and recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "490c3b95-fbdc-4223-b285-15da793d6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITER_TMPL = PromptTemplate(\n",
    "    input_variables=[\"company\",\"section\",\"goals\",\"evidence\",\"tables_plan\",\"tokens_target\"],\n",
    "    template=(\n",
    "        \"Write the section as Markdown for an ESG report compliant with ESRS, professional narrative, \"\n",
    "        \"2 to 4 paragraphs plus if useful a compact table or bullet list. \"\n",
    "        \"Explicitly cover Actual impacts and Potential impacts. \"\n",
    "        \"Weave quantitative and qualitative evidence from the provided context only. \"\n",
    "        \"Conclude with Gaps, and Recommendations with 2 to 3 items.\\n\\n\"\n",
    "        \"Company: {company}\\nSection: {section}\\nGoals: {goals}\\n\"\n",
    "        \"Tables allowable: {tables_plan}\\n\"\n",
    "        \"Context evidence:\\n{evidence}\\n\\n\"\n",
    "        \"Target tokens: {tokens_target}\\n\"\n",
    "        \"Output only the Markdown for this section.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ac30b4-3384-4d7a-9a3c-5d2364c15694",
   "metadata": {},
   "source": [
    "## Query Analyzer Prompt\n",
    "\n",
    "Prompt template for the LLM to turn a section’s intent into optimized retrieval queries and simple metadata filters, returned as JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96798fab-b4d7-41f0-becf-59d2aa42da2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_ANALYZER_TMPL = PromptTemplate(\n",
    "    input_variables=[\"section_title\",\"goals\"],\n",
    "    template=(\n",
    "        \"Convert the section intent to 2 retrieval queries and 1 boolean filter suggestion over metadata \"\n",
    "        \"(e.g., source contains xlsx or docx, or page range). Keep queries concise.\\n\"\n",
    "        \"Section: {section_title}\\nGoals: {goals}\\n\"\n",
    "        \"Return JSON with keys: queries, filters\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9b8a3-fc9c-47f4-86f9-32150bb1a7e8",
   "metadata": {},
   "source": [
    "## Input Files\n",
    "\n",
    "Locate the DOCX and XLSX input files in the `data` directory using filename patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "258e45be-ed19-49ff-8581-1fb4c8f3f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect input files\n",
    "DOCX_PATH = next(DATA_DIR.glob(\"TO_SHARE_ai_engineering_challenge_company_descriptions*.docx\"))\n",
    "XLSX_PATH = next(DATA_DIR.glob(\"TO_SHARE_insights_extract_ai_engineering_challenge*.xlsx\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec468e4-960e-41f3-a6d2-4d695cb9e5ff",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "For each company:\n",
    "1. Extract description and insights  \n",
    "2. Build documents and vector store  \n",
    "3. Generate full ESG report (Markdown)  \n",
    "4. Save outputs as both `.md` and `.pdf`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90b41792-f04c-4d1c-9bc7-b0fa60f00381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Building ESG report for HyperDrive Logistics ===\n",
      "Saved Markdown to reports/report_hyperdrivelogistics.md\n",
      "Saved PDF to reports/report_hyperdrivelogistics.pdf\n",
      "\n",
      "=== Building ESG report for Galactic Donuts ===\n",
      "Saved Markdown to reports/report_galacticdonuts.md\n",
      "Saved PDF to reports/report_galacticdonuts.pdf\n",
      "\n",
      "=== Building ESG report for StellarForge Events ===\n",
      "Saved Markdown to reports/report_stellarforgeevents.md\n",
      "Saved PDF to reports/report_stellarforgeevents.pdf\n"
     ]
    }
   ],
   "source": [
    "company_map = extract_company_descriptions(DOCX_PATH)\n",
    "insights_df = load_first_sheet(XLSX_PATH)\n",
    "\n",
    "for company_name, description in list(company_map.items()):\n",
    "    print(f\"\\n=== Building ESG report for {company_name} ===\")\n",
    "    docs = build_documents(company_name, description, insights_df)\n",
    "    vs = build_or_load_vs(company_name, docs, embeddings)\n",
    "    report_md = build_report(company_name, vs, industry_hint=\"Logistics and Supply Chain\")\n",
    "\n",
    "    # Save Markdown\n",
    "    path_md = save_report(company_name, report_md)\n",
    "    print(f\"Saved Markdown to {path_md}\")\n",
    "\n",
    "    # Save PDF\n",
    "    path_pdf = save_report_pdf(company_name, report_md)\n",
    "    print(f\"Saved PDF to {path_pdf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10839e36-4607-4b2b-aa96-cfa496a8d103",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
